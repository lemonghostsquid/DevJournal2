13 February 2015
I have downloaded the screen scraping program; HtmlUnit on my iMac. I went on StackOverflow to learn what to put into the HtmlUnit program. I have also begun downloading NetBeans for my iMac via the tutorial they have on their website. The first screenscrapings will begin next week and I will hopefully be able to have the code for (at least) several SCP files. This progress will help me in creating the program and I am beginning to think that I could add more features of the program. Perhaps a website would be useful for distributing the information to people on the internet. I will also have to consider the aesthetics of the website; I already have some design ideas on paper for the website’s physical appearance. I also have some design ideas for what the GUI will look like. Perhaps I will be able to make a search bar so the user can search for specific files (I still need to research how to do this, but I definitely think that a search feature would be nice).

20 February 2015
I have added additional code to the HtmlUnit and I have begun working on the NetBeans coding. I will probably use Weebly to make my website for the program. Unfortunately I have been set back a bit because my flash drive was corrupted and I had to format it. Despite this, I will definitely be able to get some work done and scrape more files. Also, I have decided to move all of my 20% Project work to a Windows computer, since the Mac OS X is a little bit less friendly to the necessary programs. All of my stuff is now on the windows computer and hopefully there will be less problems regarding compatibility. I have envisioned the GUI having the following: a working search box, anywhere from 200-500 separate SCP files, and a README.txt file that will explain the SCP foundation, classifications, and the actual website. The README.txt will be written to tell the user how to use the program.

27 February 2015
I have created the website via the Weebly website creator, complete with links and information about the whole concept behind the program. I used a cool aesthetic, visual theme that matches the classified, official motif of the SCP Foundation. The website features three separate sections: home, about, and contact (home is the place where the download file will be). The website URL is http://scpfile.weebly.com/. I do realize that it will not be until the end of the project that I will be able to actually have the program on the website, but I am sure that having the website now is better than having it later since it is a fairly easy thing to do. Another thing that I have done is explore the NetBeans Program. Unsurprisingly, the program itself has a fairly complex GUI and I am in the process of exploring all of its features and understanding how it works. I have some pretty great ideas for the program’s GUI.

6 March 2015
I have completed my layout draft of the GUI and it has been turned in and everything. Unfortunately, I have run into some issues with my scraping program HTMLUnitScraper. I am going to ask for some help with it on Monday, and I may have to look into other screen scraping options. WebHarvy and NekoHTML seem like viable options, but I am only going to switch if I absolutely have to. Much of the problem comes from the fact that HTMLUnitScraper lacks a GUI and its file seem to be mainly Internet Explorer web pages. There are literally hundreds of the web pages, and they do not provide any easy directions. The website that I got HTMLUnitScraper from is not very helpful either. Despite these drawbacks, I am sure that all of the necessary work will be done by next Friday, and I am looking forward to asking for help on Monday.

13 March 2015
I have found a completely new program called Jtidy that claims to be simpler and better to use. It seemed that HTMLUnit was made for programmers who have had lots of experience with coding. Jtidy took half as long to download and seems to have much less files. This is good, because there is less stuff that I might mess up. Also, I have decided that the project will only include the first series of SCPs, which would be the first 999 SCP files. I think that 2999 is probably too much to use, and it would greatly lengthen the time it takes to download the file. I recent issue that I have run into is the parts of text that are considered REDACTED. These confidential pieces of information are represented by solid black lines, and I fear that java will not be able to process them (much like how it cannot process certain accented characters).

20 March 2015
Mr. Schreiber was not here on 20% Project day (today, Friday), so I did not get to ask for advice on the scraping programs. I will have to ask on the next Friday that I see him (in two weeks). To better understand the concept of screen scraping, I went to technopedia to find out more. I understand that it is a way to collect screen display data from an application and translate it so that another application can display it. In my case, I will have to “web scrape” from the SCP wiki to get all the data that I need. I assume that the website in question uses the HTML format, since it is a wiki. If I cannot properly scrape the data, I will probably have to ask for help on Piazza, Yahoo Answers, or StackOverflow or simply copy and paste the website data into the program itself.
Mr. Schreiber was not here on 20% Project day (today, friday), so I did not get to ask for advice on the scraping programs.

3 April 2015
Today, Mr. Schreiber helped me figure out how to properly take data from the websites. It turns out that BlueJ actually has a highly useful features built into it: java.net.* and java.io.*. I can use both of these to screen scrape the website, and I have already created the BlueJ java class called URLReader. It compiles completely and it even has the URL http://www.scp-wiki.net/. Today I uploaded this code to Git Hub. Mr. Schreiber also helped me learn how to use the JTidy program. I now know how exactly JTidy is used, and I can finally use its Internet Explorer files to create a full Java class, which will work with the class that I made today. At any rate, today was indeed a great success. Hopefully I will have much more work by the next Friday. Now that I finally have my screen scraping program figured out, I can actually accomplish more things with this project.

10 April 2015
So I messed around with the URLReader today and it produced surprising results. It made a Terminal Window with loads and loads of data. Much of it seems to be enclosed in angle brackets otherwise known as “chevrons”. Unfortunately, it did not contain any of the desired SCP data; it only had code for buttons and links on the home page. I posted the results in a notepad file on GitHub. I was curious as to what would occur if I were to use the URL of a specific SCP file on the wiki, so I experimented with the code. Instead of http://www.scp-wiki.net/, I used http://www.scp-wiki.net/scp-173 in URLReader. Unfortunately it seemed to produce the same results as the original; it lacked any useful information about the object, and merely showed the angle brackets and long lines of words. This coming week, I will have to ask Mr. Schreiber what I should do from here.

17 April 2015
Today I asked Mr. Schreiber for help with the URLReader. It was giving only part of the information on the page. It turned out that java was running out of buffer space, thus the URLReader main method had to be run more than once. I also learned that if you right click on a website, you can look at its code by using View Source. This is great news because it means that I can make sure that I have all of the necessary information to create my project. It seems that the main has to be run only twice to get all of the page’s information. There seems to be quite a few URLs in the source code for the website, so I think it is likely that my program will require internet to actually work. Furthermore, I think that some of the information in the code is unnecessary (i.e. links to social media websites) so I might want to consider removing them.Mr. Schreiber introduced me to a useful program called DocumentBuilder, which will help parse the information that I find. He also showed me that the information that I would want would be located at  <div id="page-content">.

24 April 2015
This week, I located the DocumentBuilder API, and it seems that I will have to make the program in java by myself, since there is no download. I found the API at this URL: http://docs.oracle.com/javase/1.5.0/docs/api/javax/xml/parsers/DocumentBuilder.html
I can understand much of the API, but there seems to be some parts where I will definitely need some help. I will ask about this next week. In the meantime, I have encountered the problem of scraping all of the SCP files. I am not sure if I have to get the data from each individual website, or if there is some easy way around this problem. If the first turns out to be true, I will probably only be able to get data from the first 100 SCPs. If there is a way to supersede the issue, I will be able to get the data of many more SCPs. I really do not want to go through each one individually.
